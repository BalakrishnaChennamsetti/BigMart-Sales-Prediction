# -*- coding: utf-8 -*-
"""BigMart Sales Prediction -Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CRkfNaEVSRy-jvOYQusRV23FIj9zZSoI
"""

"""
BigMart Sales Prediction
Author: ChatGPT
Description: End-to-end script to preprocess data, train LightGBM/XGBoost/RandomForest models,
perform cross-validation, and create submission file for the BigMart sales prediction problem.

Usage: Place 'train.csv' and 'test.csv' in the same folder as this script and run:
    python bigmart_sales_prediction.py

Outputs:
 - submission.csv (Item_Identifier, Outlet_Identifier, Item_Outlet_Sales)
 - model_cv_results.txt (CV RMSE for models)

Notes:
 - Requires: numpy, pandas, scikit-learn, lightgbm, xgboost
 - If a package is missing, install via pip, e.g. `pip install lightgbm xgboost`
"""

import warnings
warnings.filterwarnings('ignore')

import os
import sys
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import GridSearchCV

# Try imports for boosted models; instruct user if missing
try:
    import lightgbm as lgb
except Exception:
    lgb = None
try:
    import xgboost as xgb
except Exception:
    xgb = None

# -----------------------------
# Utility functions
# -----------------------------

def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

# -----------------------------
# Load data
# -----------------------------
TRAIN = 'train.csv'
TEST = 'test.csv'

if not os.path.exists(TRAIN) or not os.path.exists(TEST):
    print("Error: train.csv and test.csv must exist in the current directory.")
    sys.exit(1)

train = pd.read_csv(TRAIN)
test = pd.read_csv(TEST)

print(f"Train shape: {train.shape}, Test shape: {test.shape}")

# Keep copy of test ids
test_ids = test[['Item_Identifier','Outlet_Identifier']].copy()

# -----------------------------
# Basic feature engineering (classic BigMart recipe)
# -----------------------------

data = pd.concat([train.drop(columns=['Item_Outlet_Sales']), test], axis=0, ignore_index=True)

# 1. Item_Fat_Content cleaning
data['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF':'Low Fat','low fat':'Low Fat','reg':'Regular','Non-Edible':'Non-Edible'})
# Some non-consumables labeled 'Low Fat' etc. We'll mark them 'Non-Edible' later

# 2. Extract Item Type from Item_Identifier prefix
# Known prefixes: 'FD' -> Food, 'DR' -> Drinks, 'NC' -> Non-Consumable
data['Item_Type_Combined'] = data['Item_Identifier'].str[:2]
data['Item_Type_Combined'] = data['Item_Type_Combined'].map({'FD':'Food','DR':'Drinks','NC':'Non-Consumable'})

# 3. Impute Item_Weight by mean weight per Item_Identifier
data['Item_Weight'] = data.groupby('Item_Identifier')['Item_Weight'].transform(lambda x: x.fillna(x.mean()))
# If still NA (all missing for that Item_Identifier), fill with global mean
data['Item_Weight'].fillna(data['Item_Weight'].mean(), inplace=True)

# 4. Replace zero Item_Visibility by mean of that item
# zero likely indicates missing; replace by mean visibility per item
zero_vis_mask = (data['Item_Visibility'] == 0)
mean_vis_per_item = data.loc[~zero_vis_mask].groupby('Item_Identifier')['Item_Visibility'].transform('mean')
# For items where mean is nan, fallback to global mean
global_vis_mean = data.loc[data['Item_Visibility']>0,'Item_Visibility'].mean()

# Fill zero vis with per-item mean where available else global mean
for idx in data[zero_vis_mask].index:
    item = data.at[idx,'Item_Identifier']
    per_item_mean = data.loc[data['Item_Identifier']==item,'Item_Visibility'].mean()
    if np.isnan(per_item_mean) or per_item_mean==0:
        data.at[idx,'Item_Visibility'] = global_vis_mean
    else:
        data.at[idx,'Item_Visibility'] = per_item_mean

# 5. Create Outlet age feature (data is from 2013)
data['Outlet_Years'] = 2013 - data['Outlet_Establishment_Year']

# 6. Fill missing Outlet_Size with mode per Outlet
data['Outlet_Size'] = data.groupby('Outlet_Identifier')['Outlet_Size'].transform(lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else np.nan))
# If still NA, fill with overall mode
data['Outlet_Size'].fillna(data['Outlet_Size'].mode().iloc[0], inplace=True)

# 7. For non-consumable items, set Item_Fat_Content to 'Non-Edible'
mask_non_consumable = (data['Item_Type_Combined']=='Non-Consumable')
data.loc[mask_non_consumable,'Item_Fat_Content'] = 'Non-Edible'

# 8. Create price per unit categories by binning Item_MRP
# We can create price bins to capture non-linear relationships
data['MRP_Band'] = pd.qcut(data['Item_MRP'], 4, labels=False)

# 9. Label encode categorical variables
label_enc_cols = ['Item_Identifier','Item_Fat_Content','Item_Type','Outlet_Identifier',
                  'Outlet_Size','Outlet_Location_Type','Outlet_Type','Item_Type_Combined']
for col in label_enc_cols:
    data[col] = data[col].astype(str)
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])

# 10. Feature list
# Drop columns not useful
cols_to_drop = ['Outlet_Establishment_Year']
features = [c for c in data.columns if c not in cols_to_drop]

# Split back to train/test
train_rows = train.shape[0]
train_X = data.iloc[:train_rows].copy()
train_y = train['Item_Outlet_Sales'].copy()
test_X = data.iloc[train_rows:].copy()

# Align features (remove ID columns from features used in modeling)
model_features = [c for c in train_X.columns if c not in ['Item_Identifier','Outlet_Identifier']]

print(f"Using {len(model_features)} features for modeling")

# Log-transform target to stabilize variance
train_y_log = np.log1p(train_y)

# -----------------------------
# Cross-validation and model training
# -----------------------------

kf = KFold(n_splits=5, shuffle=True, random_state=42)

cv_results = {}

!pip install -q bayesian-optimization

from bayes_opt import BayesianOptimization

# Helper function for cross-validated RMSE
def cv_rmse(model, X, y):
    oof_preds = np.zeros(len(y))
    for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):
        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]
        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]
        model.fit(X_tr, y_tr)
        oof_preds[val_idx] = model.predict(X_val)
    return -rmse(y, oof_preds)

# 1) Random Forest with Bayesian Optimization
def rf_cv(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features):
    # Map the numerical max_features from BO to 'sqrt' or 'log2'
    max_features_str = 'sqrt' if max_features < 0.5 else 'log2'
    model = RandomForestRegressor(
        n_estimators=int(n_estimators),
        max_depth=None if max_depth < 1 else int(max_depth),
        min_samples_split=int(min_samples_split),
        min_samples_leaf=int(min_samples_leaf),
        max_features=max_features_str, # Use the corrected string value
        random_state=42, n_jobs=-1
    )
    return cv_rmse(model, train_X[model_features], train_y_log)

rf_bo = BayesianOptimization(
    f=rf_cv,
    pbounds={
        'n_estimators': (100, 300),
        'max_depth': (0, 30),
        'min_samples_split': (2, 10),
        'min_samples_leaf': (1, 4),
        'max_features': (0, 1) # Bayesian optimization will explore values between 0 and 1
    },
    random_state=42
)
rf_bo.maximize(init_points=5, n_iter=20)

# Best RF model
rf_params = rf_bo.max['params']
# Apply the same mapping for max_features for the final model
max_features_best_str = 'sqrt' if rf_params['max_features'] < 0.5 else 'log2'
rf_best = RandomForestRegressor(
    n_estimators=int(rf_params['n_estimators']),
    max_depth=None if rf_params['max_depth'] < 1 else int(rf_params['max_depth']),
    min_samples_split=int(rf_params['min_samples_split']),
    min_samples_leaf=int(rf_params['min_samples_leaf']),
    max_features=max_features_best_str, # Use the corrected string value
    random_state=42, n_jobs=-1
)
rf_oof = np.zeros(train_rows)
rf_test_preds = np.zeros(test_X.shape[0])
for fold, (tr_idx, val_idx) in enumerate(kf.split(train_X)):
    X_tr, X_val = train_X.iloc[tr_idx][model_features], train_X.iloc[val_idx][model_features]
    y_tr, y_val = train_y_log.iloc[tr_idx], train_y_log.iloc[val_idx]
    rf_best.fit(X_tr, y_tr)
    rf_oof[val_idx] = rf_best.predict(X_val)
    rf_test_preds += rf_best.predict(test_X[model_features]) / kf.n_splits
rf_rmse = rmse(train_y_log, rf_oof)
cv_results['RandomForest'] = rf_rmse
print(f"RandomForest CV RMSE (log-target): {rf_rmse:.5f}")

# 2) LightGBM with Bayesian Optimization
def lgb_cv(num_leaves, max_depth, learning_rate, n_estimators):
    model = lgb.LGBMRegressor(
        objective='regression', metric='rmse', verbosity=-1,
        num_leaves=int(num_leaves),
        max_depth=-1 if max_depth < 0 else int(max_depth),
        learning_rate=learning_rate,
        n_estimators=int(n_estimators),
        random_state=42
    )
    return cv_rmse(model, train_X[model_features], train_y_log)

lgb_bo = BayesianOptimization(
    f=lgb_cv,
    pbounds={
        'num_leaves': (20, 150),
        'max_depth': (-1, 20),
        'learning_rate': (0.01, 0.2),
        'n_estimators': (500, 1500)
    },
    random_state=42
)
lgb_bo.maximize(init_points=5, n_iter=20)

lgb_params = lgb_bo.max['params']
lgb_best = lgb.LGBMRegressor(
    objective='regression', metric='rmse', verbosity=-1,
    num_leaves=int(lgb_params['num_leaves']),
    max_depth=-1 if lgb_params['max_depth'] < 0 else int(lgb_params['max_depth']),
    learning_rate=lgb_params['learning_rate'],
    n_estimators=int(lgb_params['n_estimators']),
    random_state=42
)

lgb_oof = np.zeros(len(train_y_log))
lgb_test_preds = np.zeros(test_X.shape[0])
for fold, (tr_idx, val_idx) in enumerate(kf.split(train_X)):
    X_tr, X_val = train_X.iloc[tr_idx][model_features], train_X.iloc[val_idx][model_features]
    y_tr, y_val = train_y_log.iloc[tr_idx], train_y_log.iloc[val_idx]
    lgb_best.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],
                 callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]) # Use callbacks for early stopping
    lgb_oof[val_idx] = lgb_best.predict(X_val)
    lgb_test_preds += lgb_best.predict(test_X[model_features]) / kf.n_splits

cv_results['LightGBM'] = rmse(train_y_log, lgb_oof)

# 3) XGBoost with Bayesian Optimization
def xgb_cv(max_depth, learning_rate, n_estimators, subsample, colsample_bytree):
    model = xgb.XGBRegressor(
        objective='reg:squarederror', eval_metric='rmse',
        max_depth=int(max_depth),
        learning_rate=learning_rate,
        n_estimators=int(n_estimators),
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        random_state=42, n_jobs=-1
    )
    return cv_rmse(model, train_X[model_features], train_y_log)

xgb_bo = BayesianOptimization(
    f=xgb_cv,
    pbounds={
        'max_depth': (3, 10),
        'learning_rate': (0.01, 0.2),
        'n_estimators': (500, 1500),
        'subsample': (0.7, 1.0),
        'colsample_bytree': (0.7, 1.0)
    },
    random_state=42
)
xgb_bo.maximize(init_points=5, n_iter=20)

xgb_params = xgb_bo.max['params']
xgb_best = xgb.XGBRegressor(
    objective='reg:squarederror', eval_metric='rmse',
    max_depth=int(xgb_params['max_depth']),
    learning_rate=xgb_params['learning_rate'],
    n_estimators=int(xgb_params['n_estimators']),
    subsample=xgb_params['subsample'],
    colsample_bytree=xgb_params['colsample_bytree'],
    random_state=42, n_jobs=-1
)

xgb_oof = np.zeros(len(train_y_log))
xgb_test_preds = np.zeros(test_X.shape[0])
for fold, (tr_idx, val_idx) in enumerate(kf.split(train_X)):
    X_tr, X_val = train_X.iloc[tr_idx][model_features], train_X.iloc[val_idx][model_features]
    y_tr, y_val = train_y_log.iloc[tr_idx], train_y_log.iloc[val_idx]
    xgb_best.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)
    xgb_oof[val_idx] = xgb_best.predict(X_val)
    xgb_test_preds += xgb_best.predict(test_X[model_features]) / kf.n_splits

cv_results['XGBoost'] = rmse(train_y_log, xgb_oof)

lgb_params = lgb_bo.max['params']
xgb_params = xgb_bo.max['params']

lgb_best = lgb.LGBMRegressor(
    objective='regression', metric='rmse',
    num_leaves=int(lgb_params['num_leaves']),
    max_depth=-1 if lgb_params['max_depth'] < 0 else int(lgb_params['max_depth']),
    learning_rate=lgb_params['learning_rate'],
    n_estimators=int(lgb_params['n_estimators']),
    random_state=42
)

xgb_best = xgb.XGBRegressor(
    objective='reg:squarederror', eval_metric='rmse',
    max_depth=int(xgb_params['max_depth']),
    learning_rate=xgb_params['learning_rate'],
    n_estimators=int(xgb_params['n_estimators']),
    subsample=xgb_params['subsample'],
    colsample_bytree=xgb_params['colsample_bytree'],
    random_state=42, n_jobs=-1
)

# -----------------------------
# Save CV results
# -----------------------------
with open('model_cv_results.txt', 'w') as f:
    for k, v in cv_results.items():
        f.write(f"{k}: {v}\n")

# -----------------------------
# Choose ensemble of available models (simple average) and create submission
# -----------------------------
preds = np.zeros(test_X.shape[0])
count = 0

if 'RandomForest' in cv_results:
    preds += rf_test_preds
    count += 1

if lgb is not None and 'LightGBM' in cv_results:
    preds += lgb_test_preds
    count += 1

if xgb is not None and 'XGBoost' in cv_results:
    preds += xgb_test_preds
    count += 1

if count > 0:
    preds /= count
else:
    print("No models were trained successfully. Cannot create submission.")
    sys.exit(1)

# Inverse transform log-target predictions
final_preds = np.expm1(preds)

# -----------------------------
# Match sample submission format & order
# -----------------------------
sample_path = "sample_submission_8RXa3c6 (1).csv"  # path to your provided sample file
sample_sub = pd.read_csv(sample_path)

# Ensure correct order by aligning predictions to the sample IDs
submission = sample_sub.copy()
submission["Item_Outlet_Sales"] = final_preds

submission.to_csv("submission.csv", index=False)
print("Saved submission.csv with exact sample format")

# -----------------------------
# Quick feature importance from RandomForest
# -----------------------------
try:
    if 'RandomForest' in cv_results:
        importances = rf_best.feature_importances_
        feat_imp = pd.DataFrame({'feature': model_features, 'importance': importances}) \
                      .sort_values('importance', ascending=False)
        feat_imp.to_csv('feature_importances.csv', index=False)
        print('Saved feature_importances.csv')
except Exception as e:
    print(f"Could not save feature importances: {e}")

print('Done')

# -----------------------------
# Quick feature importance from RandomForest
# -----------------------------
try:
    importances = rf_best.feature_importances_
    feat_imp = pd.DataFrame({'feature':model_features,'importance':importances}).sort_values('importance',ascending=False)
    feat_imp.to_csv('feature_importances.csv', index=False)
    print('Saved feature_importances.csv')
except Exception:
    pass

print('Done')

